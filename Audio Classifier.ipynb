{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import keras\n",
    "from keras.layers import Activation, Dense, Dropout, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers import GaussianNoise\n",
    "\n",
    "from keras import optimizers\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution\n",
    "category_group=train.groupby(['label']).count()\n",
    "print(type(category_group))\n",
    "plot=category_group.unstack().plot(kind='bar', stacked=True, title=\"Number of Audio Samples per Category\", figsize=(16,10))\n",
    "plot.set_xlabel(\"Category\")\n",
    "plot.set_ylabel(\"Number of Samples\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd  # To play sound in the notebook\n",
    "fname1 = './wav/' + 'cc499e63eee4a3bcca48b5b452df04990df83570.wav'   # Hi-hat\n",
    "ipd.Audio(fname1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting audio frame\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.plot(data, '-', );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(wav)), ref=np.max)\n",
    "librosa.display.specshow(D, y_axis='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract training mfcc (20 features) using training dataframe and insert it into train\n",
    "def extract_mfcc(paths):\n",
    "    result = []\n",
    "    for i in paths:\n",
    "        y, sr = librosa.load('./wav/wav/{}'.format(i), sr = 22050)\n",
    "        ps = librosa.feature.mfcc(y=y, sr= sr, n_mfcc=20)\n",
    "        result.append((ps))\n",
    "    return result\n",
    "\n",
    "trainfiles = list(train[\"path\"])\n",
    "testfiles = list(test[\"path\"])\n",
    "\n",
    "train[\"mfcc\"] = extract_mfcc(trainfiles)\n",
    "test[\"mfcc\"] = extract_mfcc(testfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add padding for uncommon shapes\n",
    "def add_padding(mfccs):\n",
    "    result = []\n",
    "    for i in mfccs:\n",
    "        if i.shape[1] != 44:\n",
    "            i = librosa.util.fix_length(i, 44)\n",
    "            result.append((i.T))\n",
    "        else:\n",
    "            result.append((i.T))\n",
    "    return result\n",
    "\n",
    "train[\"feature\"] = add_padding(train[\"mfcc\"])\n",
    "train = train.drop(\"mfcc\", axis = 1)\n",
    "\n",
    "test[\"feature\"] = add_padding(test[\"mfcc\"])\n",
    "test = test.drop(\"mfcc\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#split training data into training and validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(train[\"feature\"], train[\"word\"], test_size=0.2, random_state=288)\n",
    "\n",
    "#reshape training and validation data into 44 by 20 by 1\n",
    "X_train = np.array([x.reshape( (44,20,1)) for x in X_train])\n",
    "X_test = np.array([x.reshape( (44,20,1)) for x in X_test])\n",
    "X_predict = np.array([x.reshape( (44,20,1)) for x in test[\"feature\"]])\n",
    "\n",
    "#Change words to numerical value\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "#Change numerical value of words to matrix representation\n",
    "y_train = np.array(keras.utils.to_categorical(y_train,35))\n",
    "y_test = np.array(keras.utils.to_categorical(y_test, 35))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify CNN model \n",
    "model = Sequential()\n",
    "\n",
    "input_shape = (44, 20, 1) #specify input shape\n",
    "\n",
    "model.add(Conv2D(16, kernel_size=(3,3), activation=\"relu\", input_shape = input_shape)) #apply a 32 filters Conv2D layer with a 3,3 kernel\n",
    "# model.add(Dropout(0.2)) #apply a randomness dropout of .3 to avoid overfitting\n",
    "model.add(BatchNormalization()) #use batchnormalization to normalize the layer input\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3,3), activation=\"relu\", input_shape = input_shape)) #apply a 32 filters Conv2D layer with a 3,3 kernel\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2))) #apply a max pooling layer to avoid overfitting\n",
    "model.add(Dropout(0.2)) #apply a randomness dropout of .3 to avoid overfitting\n",
    "model.add(BatchNormalization()) #use batchnormalization to normalize the layer input\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation=\"relu\")) #apply a 64 filters Conv2D layer with a 3,3 kernel\n",
    "model.add(Dropout(0.2)) #apply a randomness dropout of .3 to avoid overfitting\n",
    "model.add(BatchNormalization()) #use batchnormalization to normalize the layer input\n",
    "\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(5,5), activation=\"relu\")) #apply a 64 filters Conv2D layer with a 3,3 kernel\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2))) #apply a max pooling layer to avoid overfitting\n",
    "model.add(Dropout(0.2)) #apply a randomness dropout of .3 to avoid overfitting\n",
    "model.add(BatchNormalization()) #use batchnormalization to normalize the layer input\n",
    "\n",
    "model.add(GaussianNoise(0.1)) #apply a gaussian noise layer to account for noise\n",
    "\n",
    "model.add(Flatten()) #apply a flatten layer\n",
    "\n",
    "Dense(105, activation = \"relu\")\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(35, activation=\"softmax\")) #apply a dense layer of 35 to get the data in the right output shape\n",
    "\n",
    "\n",
    "\n",
    "#Define the setup of the model by using Adam as optimizer, categorical crossentropy for analysing loss and accuracy for measuring performance\n",
    "model.compile(\n",
    "        optimizer = \"Adam\",\n",
    "        loss = \"categorical_crossentropy\",\n",
    "        metrics = ['accuracy'])\n",
    "\n",
    "#Fitting the model on our data\n",
    "history = model.fit(x = X_train, y = y_train, epochs = 20, batch_size = 100, validation_data=(X_test, y_test))\n",
    "\n",
    "#Give a summary of our model\n",
    "model.summary()\n",
    "\n",
    "#Plot the accuracy for every epoch\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#Plot the loss for every epoch\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#Plot the score of our model\n",
    "score = model.evaluate(x = X_test, y = y_test)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit our model on the test data\n",
    "y_predict = model.predict(X_predict)\n",
    "\n",
    "#Decode our results to words\n",
    "results = []\n",
    "for i in y_predict:\n",
    "    x = np.argmax(i) #get position of highest value\n",
    "    results += [x]\n",
    "    \n",
    "results = le.inverse_transform(results) #get word for the numerical position\n",
    "\n",
    "test[\"word\"] = results #insert into test dataframe\n",
    "\n",
    "test = test.drop(\"feature\", axis = 1)\n",
    "test.to_csv('test_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
